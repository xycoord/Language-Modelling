experiment_name: "small_transformer"

seed: 1337
learning_rate: 3.0e-4
batch_size: 64
epochs: 2
block_size: 64
vocab_size: 65
max_train_steps: 5000
eval_interval: 100
example_interval: 100
train_split: 0.95
compile_model: false
mixed_precision: true
tokenizer_path: "data/tokenizers/shakespeare_tokenizer.json"

inherits:
  - configs/base/model_architectures/small_transformer.yaml
  - configs/base/system/laptop.yaml